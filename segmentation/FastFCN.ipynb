{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3dd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d26693b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_kwargs = {'mode': 'bilinear', 'align_corners': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fc4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=False, norm_layer=nn.BatchNorm2d):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation, groups=inplanes, bias=bias)\n",
    "        self.bn = norm_layer(inplanes)\n",
    "        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2617cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JPU(nn.Module):\n",
    "    def __init__(self, in_channels, width=512, norm_layer=None, up_kwargs=None):\n",
    "        super(JPU, self).__init__()\n",
    "        self.up_kwargs = up_kwargs\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n",
    "            norm_layer(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n",
    "            norm_layer(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n",
    "            norm_layer(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        self.dilation1 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=1, dilation=1, bias=False),\n",
    "                                       norm_layer(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation2 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=2, dilation=2, bias=False),\n",
    "                                       norm_layer(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation3 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=4, dilation=4, bias=False),\n",
    "                                       norm_layer(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation4 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=8, dilation=8, bias=False),\n",
    "                                       norm_layer(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]), self.conv3(inputs[-3])]\n",
    "        _, _, h, w = feats[-1].size()\n",
    "        feats[-2] = F.interpolate(feats[-2], (h, w), **self.up_kwargs)\n",
    "        feats[-3] = F.interpolate(feats[-3], (h, w), **self.up_kwargs)\n",
    "        feat = torch.cat(feats, dim=1)\n",
    "        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat), self.dilation4(feat)], dim=1)\n",
    "\n",
    "        return inputs[0], inputs[1], inputs[2], feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0441bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, nclass, backbone, aux, se_loss, jpu=True, dilated=False, norm_layer=None,\n",
    "                 base_size=520, crop_size=480, mean=[.485, .456, .406],\n",
    "                 std=[.229, .224, .225], root='~/.encoding/models', **kwargs):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.nclass = nclass\n",
    "        self.aux = aux\n",
    "        self.se_loss = se_loss\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        # copying modules from pretrained models\n",
    "        if backbone == 'resnet50':\n",
    "            self.pretrained = models.resnet50(pretrained=False)\n",
    "        elif backbone == 'resnet101':\n",
    "            self.pretrained = models.resnet101(pretrained=False)\n",
    "        elif backbone == 'resnet152':\n",
    "            self.pretrained = models.resnet152(pretrained=False)\n",
    "        else:\n",
    "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
    "        # bilinear upsample options\n",
    "        self._up_kwargs = up_kwargs\n",
    "        self.backbone = backbone\n",
    "        self.jpu = None\n",
    "        if jpu == 'JPU':\n",
    "            self.jpu = JPU([512, 1024, 2048], width=512, norm_layer=norm_layer, up_kwargs=up_kwargs)\n",
    "        elif jpu == 'JPU_X':\n",
    "            self.jpu = JPU_X([512, 1024, 2048], width=512, norm_layer=norm_layer, up_kwargs=up_kwargs)\n",
    "            \n",
    "    def base_forward(self, x):\n",
    "        x = self.pretrained.conv1(x)\n",
    "        x = self.pretrained.bn1(x)\n",
    "        x = self.pretrained.relu(x)\n",
    "        x = self.pretrained.maxpool(x)\n",
    "        c1 = self.pretrained.layer1(x)\n",
    "        c2 = self.pretrained.layer2(c1)\n",
    "        c3 = self.pretrained.layer3(c2)\n",
    "        c4 = self.pretrained.layer4(c3)\n",
    "\n",
    "        if self.jpu:\n",
    "            return self.jpu(c1, c2, c3, c4)\n",
    "        else:\n",
    "            return c1, c2, c3, c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a58cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNHead(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm_layer):\n",
    "        super(FCNHead, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                   norm_layer(inter_channels),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Dropout2d(0.1, False),\n",
    "                                   nn.Conv2d(inter_channels, out_channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64962dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(BaseNet):\n",
    "    def __init__(self, nclass, backbone, aux=True, se_loss=False, norm_layer=nn.BatchNorm2d, **kwargs):\n",
    "        super(FCN, self).__init__(nclass, backbone, aux, se_loss, norm_layer=norm_layer, **kwargs)\n",
    "        self.head = FCNHead(2048, nclass, norm_layer)\n",
    "        if aux:\n",
    "            self.auxlayer = FCNHead(1024, nclass, norm_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        imsize = x.size()[2:]\n",
    "        _, _, c3, c4 = self.base_forward(x)\n",
    "\n",
    "        x = self.head(c4)\n",
    "        x = F.interpolate(x, imsize, **self._up_kwargs)\n",
    "        outputs = [x]\n",
    "        if self.aux:\n",
    "            auxout = self.auxlayer(c3)\n",
    "            auxout = F.interpolate(auxout, imsize, **self._up_kwargs)\n",
    "            outputs.append(auxout)\n",
    "        return tuple(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3df41fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = FCN(nclass=2, backbone=\"resnet50\", aux=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80ce64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.randn((1, 3, 256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3be6099c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7761/1384306181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b3c7c6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gather() received an invalid combination of arguments - got (tuple, int, int), but expected one of:\n * (Tensor input, int dim, Tensor index, *, bool sparse_grad, Tensor out)\n * (Tensor input, name dim, Tensor index, *, bool sparse_grad, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7761/3041653755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: gather() received an invalid combination of arguments - got (tuple, int, int), but expected one of:\n * (Tensor input, int dim, Tensor index, *, bool sparse_grad, Tensor out)\n * (Tensor input, name dim, Tensor index, *, bool sparse_grad, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.gather(outputs, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714c5e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 256, 256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dae6acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel.scatter_gather import gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f3647db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Gather function not implemented for CPU tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7761/3157707586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Setting the function to None clears the refcycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgather_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mgather_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mgather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mGather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         assert all(i.device.type != 'cpu' for i in inputs), (\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;34m'Gather function not implemented for CPU tensors'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_device\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Gather function not implemented for CPU tensors"
     ]
    }
   ],
   "source": [
    "gather(outputs, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e6a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv1",
   "language": "python",
   "name": "youtube-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
